---
author: Matt Palmer
description: "The best ways I've found to write clean, partitioned datasets in just a few linesâ€” with Pandas, PyArrow, Polars, and DuckDB."
draft: false
featured: true
ogImage: ""
postSlug: partitioned-parquet-part-1
pubDatetime: 2024-02-19
tags: [opinion]
title: Writing Partitioned Parquet (Part 1)
emoji: 1ï¸âƒ£
---

One of the most complicated things about data engineering is that there are _very few_ problems with a industry-accepted solution: thereâ€™s no â€œrightâ€ way to build out an infrastructure or solve a problem.

Even when there is a concensus, itâ€™s really hard to find out about it (this is a big reason why you should talk to people and go to conferences).

In part, thatâ€™s a good thing, because there are few universal solutions. It wouldn't make sense for a 50 person start-up to adopt Uberâ€™s data engineering architecture.

However, there _are_ a number of common data engineering tasks that almost everyone has to doâ€” reading data, transforming it, and writing it. Performance demands can be different, but for the most part we all just want to _simplify_ and _optimize._

In this article, Iâ€™ll focus on a very specific part of data engineeringâ€” writing partitioned datasets in the [parquet](https://parquet.apache.org) format. Itâ€™s something thatâ€™s come up time and time again for me and something you might find value in, regardless of your chosen solution. 

## âœï¸ The Prompt

Our task will be to read a list of failed banks from the FDIC and write it to a dataset partitioned by state. Itâ€™s a pretty small set of data, so this should be a good example. 

Our loading script:

```python
import pandas as pd
import os

url = 'https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/banklist.csv'

bank_df = pd.read_csv(url, encoding="windows-1251")

bank_df.columns = [c.strip() for c in bank_df.columns]

bank_df.head()
```

And we want to create an output like:
    
```
bank_data/
â”œâ”€â”€ State=AL
â”‚Â Â  â””â”€â”€ data_0.parquet
â”œâ”€â”€ State=AR
â”‚Â Â  â””â”€â”€ data_0.parquet
â”œâ”€â”€ State=AZ
â”‚Â Â  â””â”€â”€ data_0.parquet
...
â”œâ”€â”€ State=WI
â”‚Â Â  â””â”€â”€ data_0.parquet
â”œâ”€â”€ State=WV
â”‚Â Â  â””â”€â”€ data_0.parquet
â””â”€â”€ State=WY
    â””â”€â”€ data_0.parquet
```

Weâ€™ll walk though four options (wow, so many options)â€”DuckDB, Pandas, Polars, and PyArrowâ€”covering differences, similarities, and benchmarks.

## ğŸ¦† DuckDB

DuckDB makes this easyâ€” using SQL, we just write a `COPY` statement to export the data to a folder:
    
```python    
import duckdb

conn = duckdb.connect()

conn.sql("COPY bank_df TO 'bank_data' (FORMAT PARQUET, PARTITION_BY (State), OVERWRITE_OR_IGNORE 1)")
```

If properly configured, you can do this just as easily with a destination in your cloud provider of choice. I cover this in my in-depth article on DuckDB [here](https://newsletter.casewhen.xyz/i/139967696/one-line-partitioned-parquet).

To read the data:
    
```python
conn.sql("SELECT * FROM read_parquet('bank_data/**/*.parquet')")
```

Behind the scenes, DuckDB is streaming parquet files _while_ using parallel processing with optimizations like automatic filtering + projection pushdown. 

That means you can query larger-than memory datasets and get performance gains over single-threaded alternatives. You can read more about those concepts in a DuckDB blog [here](https://duckdb.org/2021/06/25/querying-parquet.html) from 2021.

## ğŸ¼ Pandas

Under the hood, Pandas is using [fastparquet or PyArrow](https://github.com/pandas-dev/pandas/blob/f538741432edf55c6b9fb5d0d496d2dd1d7c2457/pandas/io/parquet.py#L51). With PyArrow, we see similar performance characteristics to DuckDB:
    
```python  
bank_df.to_parquet(
    "pandas_bank_data",
    partition_cols=["State"],
)
```

As of Pandas 2.2.0, there isnâ€™t a way to scan a directory of parquet files using pattern matching, so youâ€™ll need DuckDB/Polars or a for-loop to do this easily.

## ğŸ»â€â„ï¸ Polars

Again, we note that, under the hood, Polars is using PyArrow to build our partititioned output. Still, this is a pretty neat one-liner:
    
```python    
import polars as pl

pl_bank_df = pl.from_pandas(bank_df)

pl_bank_df.write_parquet(
    "polars_bank_data",
    use_pyarrow=True,
    pyarrow_options={"partition_cols": ["State"]},
)
```

Again, this is interoperable with S3, GCP, etc. Reading our dataset:
    
```python
pl.scan_parquet("polars_bank_data/**/*.parquet").collect()
```

Simple and easy! Good news for Polars early adopters. ğŸ˜„

## ğŸ¹ PyArrow

Ok, so weâ€™ve noted that Pandas _and_ Polars both use a PyArrow backend for this operation. 

While we pickup finer-grained control with PyArrow, this is at the cost of simplicity. The PyArrow docs can be [complex](https://arrow.apache.org/docs/python/parquet.html#reading-and-writing-the-apache-parquet-format) and we have to first read the data into a PyArrow Table:

```python
import pyarrow as pa

bank_table = pa.Table.from_pandas(bank_df)
```

We can then write our output as expected:
    
```python
import pyarrow.parquet as pq

pq.write_to_dataset(bank_table, root_path='pyarrow_bank_data',
                    partition_cols=['State'])
```
And read: 
```
table = pq.read_table('pyarrow_bank_data')

## ğŸ¥‡Performance

If we isolate & execute our commands in a jupyter notebook with the `%%timeit` magic function, we can get execution times and standard deviations for each `write`.

You can find the complete notebook [here](https://gist.github.com/mattppal/5f58a9f85af0efb0566568550fa7e9ec).

## ğŸ¤” Takeaways

It shouldnâ€™t come as a huge surprise that Polars, Pandas, and PyArrow are nearly identicalâ€” theyâ€™re all using the same backend. With overlapping standard deviations, we should consider these results identical. More surprising is the performance of DuckDB.

Of course, we have to consider that almost _every_ library requires conversion before writing. 

* If you _already_ have a Pandas dataframe, youâ€™ll need to consider the time to import another library, convert, and write the dataframe. 

* Because Pandas shares the same backend as PyArrow and Polars, you should never convert a Pandas dataframe to PyArrow/Polars _just_ to write it. Thatâ€™s just adding complexity and additional overhead. 

Additionally, this is a _really_ tiny dataset. We shouldnâ€™t assume the same results for production-size workloads. But, we can draw some preliminary conclusions:

* With Pandas, if `df.parquet()` does the trick, stick with it.

* If youâ€™re using Polars, stick with Polars.

* If you need more granular control, toggling to PyArrow might be useful.

In a follow-up post, weâ€™ll explore if converting your dataframe to DuckDB allows you to write partitioned files faster. 

In that example, weâ€™ll use some real-world sized data (think GBs). Weâ€™ll also consider implications for writing to cloud storageâ€” for example, is it just as simple to write a PyArrow table as a dataframe?