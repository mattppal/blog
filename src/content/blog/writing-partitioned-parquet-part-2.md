---
author: Matt Palmer
description: "The best ways I've found to write clean, partitioned datasets in just a few linesâ€” with Pandas, PyArrow, Polars, and DuckDB."
draft: false
featured: true
ogImage: ""
postSlug: partitioned-parquet-part-2
pubDatetime: 2024-02-26
tags: [opinion]
title: Writing Partitioned Parquet (Part 2)
emoji: 2ï¸âƒ£
---

In [Part 1](https://open.substack.com/pub/casewhen/p/writing-partitioned-parquet?r=rnul&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true), we dove into the differences between DuckDB, Pandas, Polars, and PyArrow for writing datasets to a directory of partitioned parquet files. We also benchmarked each library, noting some surprising (and unsurprising) results.

Of course, that was with a tiny dataset. There were only 568 rows with 44 partitionsâ€¦ Probably not something youâ€™ll see in a day as a data engineer.

_So_ , weâ€™re back to fix that! Today, Iâ€™ll be working with the [NYC Taxi Dataset](https://mavenanalytics.io/data-playground?order=-fields.numberOfRecords) with 2019 data. This dataset has ~6 million rows with around 390 partitons (dates). This will be a bit more computationally intensive, so Iâ€™ve taken some extra precautions to be sure weâ€™re getting accurate results.

## âœï¸ The Benchmark

If youâ€™d like to see the code we used, you can check out a Gist [here](https://gist.github.com/mattppal/1e1eef95bf7cb29d1342566e5d2969dd). This was covered extensively in [Part 1](https://open.substack.com/pub/casewhen/p/writing-partitioned-parquet?r=rnul&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true) of this post, so feel free to bop on over there if youâ€™d like to get into specifics. 

Here are the differences in this benchmark:

*  **Dataset:** ~6m rows, 390 partitions vs. ~500 rows, 44 partitions
*  **Memory:** Between benchmarks, I cleared the notebook memory by deleting the large variables or closing the DuckDB connection. You can see this in the Gist.
*  **Machine:** While probably not perfect for benchmarking, I made sure to close other programs on my MacBook. I have an M1 Pro with 16GB of memory. 
  * Itâ€™s actually quite impressive how fast these operations were able to execute. I can only imagine what that would look like on a newer machine.
  *  _No wonder_ companies like MotherDuck are leaning into a hybrid execution model. If I can crunch 6m rows of data on my laptop in 10 seconds, how often will I _really_ need Spark?
  *  
To restate the benchmark from[ Part 1](https://open.substack.com/pub/casewhen/p/writing-partitioned-parquet?r=rnul&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true), weâ€™ll be reading in our dataset and converting it to a tree of partitioned parquet filesâ€” a common storage pattern for data engineers. If youâ€™re unfamiliar with Parquet, I highly recommend checking it out as an alternative to CSVs or JSON. 

We expect the output to look like:
    
```
  taxi_data/
  â”œâ”€â”€ lpep_pickup_date=2019-01-01
  â”‚Â Â  â”œâ”€â”€ data_0.parquet
  â”‚Â Â  â”œâ”€â”€ data_1.parquet
  â”‚Â Â  â”œâ”€â”€ data_2.parquet
  â”‚Â Â  â”œâ”€â”€ data_3.parquet
  â”‚Â Â  â”œâ”€â”€ data_4.parquet
  â”‚Â Â  â”œâ”€â”€ data_5.parquet
  â”‚Â Â  â”œâ”€â”€ data_6.parquet
  â”‚Â Â  â””â”€â”€ data_7.parquet
  ...
  â””â”€â”€ lpep_pickup_date=2019-12-31
      â”œâ”€â”€ data_0.parquet
      â”œâ”€â”€ data_1.parquet
      â”œâ”€â”€ data_2.parquet
      â”œâ”€â”€ data_3.parquet
      â”œâ”€â”€ data_4.parquet
      â”œâ”€â”€ data_5.parquet
      â”œâ”€â”€ data_6.parquet
      â””â”€â”€ data_7.parquet
```

## ğŸ¥‡Performance

If we isolate & execute our commands in a jupyter notebook with the `%%timeit` magic function, we can get execution times and standard deviations for each `write`. `timeit` executes 7 operations and logs the time and standard deviation, so we can be sure weâ€™re not getting anomalous data.

## ğŸ¤” Takeaways

As discussed in [Part 1](https://open.substack.com/pub/casewhen/p/writing-partitioned-parquet?r=rnul&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true), Polars and Pandas _both_ use PyArrow on the backend for this operation, so itâ€™s no surprise theyâ€™re all practically the same.

The more interesting result is DuckDB. Coming in at roughly 6.5x faster, DuckDB truly excelled at the writes. Better yet, thereâ€™s no intermediate â€œloadâ€ step for DuckDBâ€” itâ€™s not necessary to define a new variable, since you can read directly from the dataframe:

```python    
conn.sql(
    """
    COPY taxi_df TO 'duckdb_taxi_data' (FORMAT PARQUET, PARTITION_BY (lpep_pickup_date), OVERWRITE_OR_IGNORE 1);
    """
)
```

This brings us to a _new_ set of conclusions:

* If youâ€™re _not_ using DuckDB:

 * If youâ€™re happy with your performance, everything works, or you donâ€™t want to add a new libraryâ€¦. Do nothing. No need to introduce extra complexity.

 * If youâ€™re trying to speed up a pipeline and donâ€™t mind adding in an extra library, consider DuckDB for your `WRITE` operations. It will outperform PyArrow-based alternatives.

* Because Pandas shares the same backend as PyArrow and Polars, you should never convert a Pandas dataframe to PyArrow/Polars _just_ to write it. Thatâ€™s just adding complexity and additional overhead. 

If youâ€™re dead-set on PyArrow-based libraries:

* With Pandas, if `df.parquet()` does the trick, stick with it.

* If youâ€™re using Polars, stick with Polars.

* If you need more granular control, toggling to PyArrow might be useful.

So thatâ€™s it for Part 2.